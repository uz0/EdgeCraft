# PRP 2.6: Batch Map Loader with Parallel Loading

**Feature Name**: Batch Map Loading System with Caching
**Duration**: 3-4 days | **Team**: 1 developer | **Budget**: $3,000
**Status**: üìã Planned

**Dependencies**:
- PRP 2.2 (SC2MapLoader) - required
- PRP 2.3 (W3NCampaignLoader) - required
- PRP 2.5 (MapRendererCore) - required

---

## üéØ Objective

Implement BatchMapLoader that loads multiple maps in parallel with progress tracking, caching, and priority queue management. Enables efficient "Load All Maps" functionality for gallery/preview generation.

**Core Responsibility**: Load 24 maps efficiently with caching and progress feedback

---

## üìä Current State

**‚úÖ WORKING**:
- Individual map loaders (W3X, W3N, SC2Map)
- MapLoaderRegistry (format detection)
- MapRendererCore (single map rendering)

**‚ùå MISSING**:
- BatchMapLoader.ts - parallel loading orchestrator
- Map caching system (avoid re-parsing)
- Progress tracking and cancellation
- Priority queue (load important maps first)
- Memory management (unload old maps)

---

## üî¨ Research

**Source**: Best practices for parallel asset loading

**Key Findings**:
1. Use `Promise.allSettled()` for parallel loading with error isolation
2. Limit concurrency to avoid memory spikes (max 3 concurrent)
3. Cache parsed `RawMapData` (not full renders)
4. LRU (Least Recently Used) cache eviction
5. Progress tracking: `loaded / total` with per-map status

---

## üìã Definition of Done

- [ ] `BatchMapLoader.ts` created in `src/formats/maps/`
- [ ] Load multiple maps in parallel (max 3 concurrent)
- [ ] Progress tracking (per-map + overall)
- [ ] Cancellation support (abort in-progress loads)
- [ ] LRU cache (max 10 maps in memory)
- [ ] Priority queue (load by size, small first)
- [ ] Error handling (continue on individual failures)
- [ ] Load all 24 maps in <2 minutes total
- [ ] Memory limit: <4GB peak usage
- [ ] Unit tests (>80% coverage)

---

## üíª Implementation

```typescript
// src/formats/maps/BatchMapLoader.ts

import type { RawMapData } from './types';
import { MapLoaderRegistry } from './MapLoaderRegistry';

export interface MapLoadTask {
  /** Unique task ID */
  id: string;

  /** File to load */
  file: File | ArrayBuffer;

  /** File extension */
  extension: string;

  /** File size (for prioritization) */
  sizeBytes: number;

  /** Priority (higher = load first) */
  priority?: number;
}

export interface MapLoadProgress {
  /** Task ID */
  taskId: string;

  /** Load status */
  status: 'pending' | 'loading' | 'success' | 'error';

  /** Progress (0-100) */
  progress: number;

  /** Loaded map data (if success) */
  mapData?: RawMapData;

  /** Error message (if failed) */
  error?: string;

  /** Load time in ms */
  loadTimeMs?: number;
}

export interface BatchLoadResult {
  /** Overall success (true if ANY maps loaded) */
  success: boolean;

  /** Per-map results */
  results: Map<string, MapLoadProgress>;

  /** Total load time */
  totalTimeMs: number;

  /** Summary stats */
  stats: {
    total: number;
    succeeded: number;
    failed: number;
    cached: number;
  };
}

export interface BatchMapLoaderConfig {
  /** Max concurrent loads */
  maxConcurrent?: number;

  /** Max cached maps (LRU eviction) */
  maxCacheSize?: number;

  /** Progress callback */
  onProgress?: (progress: MapLoadProgress) => void;

  /** Enable caching */
  enableCache?: boolean;
}

export class BatchMapLoader {
  private config: Required<BatchMapLoaderConfig>;
  private cache: Map<string, RawMapData> = new Map();
  private cacheAccessOrder: string[] = [];
  private abortController: AbortController | null = null;

  constructor(config?: BatchMapLoaderConfig) {
    this.config = {
      maxConcurrent: config?.maxConcurrent ?? 3,
      maxCacheSize: config?.maxCacheSize ?? 10,
      onProgress: config?.onProgress ?? (() => {}),
      enableCache: config?.enableCache ?? true,
    };
  }

  /**
   * Load multiple maps in parallel
   */
  public async loadMaps(tasks: MapLoadTask[]): Promise<BatchLoadResult> {
    const startTime = performance.now();
    this.abortController = new AbortController();

    // Sort by priority (descending), then by size (ascending - small first)
    const sortedTasks = [...tasks].sort((a, b) => {
      if ((a.priority ?? 0) !== (b.priority ?? 0)) {
        return (b.priority ?? 0) - (a.priority ?? 0);
      }
      return a.sizeBytes - b.sizeBytes;
    });

    const results = new Map<string, MapLoadProgress>();

    // Initialize progress tracking
    for (const task of sortedTasks) {
      results.set(task.id, {
        taskId: task.id,
        status: 'pending',
        progress: 0,
      });
    }

    // Load in batches (max concurrent)
    const batches = this.createBatches(sortedTasks, this.config.maxConcurrent);

    let succeeded = 0;
    let failed = 0;
    let cached = 0;

    for (const batch of batches) {
      // Check for cancellation
      if (this.abortController.signal.aborted) {
        break;
      }

      const batchPromises = batch.map(async (task) => {
        // Check cache first
        if (this.config.enableCache && this.cache.has(task.id)) {
          const cachedData = this.cache.get(task.id)!;
          this.updateCacheAccess(task.id);

          results.set(task.id, {
            taskId: task.id,
            status: 'success',
            progress: 100,
            mapData: cachedData,
            loadTimeMs: 0,
          });
          this.config.onProgress(results.get(task.id)!);
          cached++;
          return;
        }

        // Update status to loading
        results.set(task.id, {
          taskId: task.id,
          status: 'loading',
          progress: 0,
        });
        this.config.onProgress(results.get(task.id)!);

        const taskStartTime = performance.now();

        try {
          const loader = MapLoaderRegistry.getLoader(task.extension);
          if (!loader) {
            throw new Error(`No loader for extension: ${task.extension}`);
          }

          const mapData = await loader.parse(task.file);
          const loadTimeMs = performance.now() - taskStartTime;

          // Add to cache
          if (this.config.enableCache) {
            this.addToCache(task.id, mapData);
          }

          results.set(task.id, {
            taskId: task.id,
            status: 'success',
            progress: 100,
            mapData,
            loadTimeMs,
          });
          this.config.onProgress(results.get(task.id)!);
          succeeded++;
        } catch (error) {
          const errorMsg = error instanceof Error ? error.message : String(error);
          results.set(task.id, {
            taskId: task.id,
            status: 'error',
            progress: 0,
            error: errorMsg,
            loadTimeMs: performance.now() - taskStartTime,
          });
          this.config.onProgress(results.get(task.id)!);
          failed++;
        }
      });

      await Promise.allSettled(batchPromises);
    }

    const totalTimeMs = performance.now() - startTime;

    return {
      success: succeeded > 0,
      results,
      totalTimeMs,
      stats: {
        total: sortedTasks.length,
        succeeded,
        failed,
        cached,
      },
    };
  }

  /**
   * Cancel all in-progress loads
   */
  public cancel(): void {
    if (this.abortController) {
      this.abortController.abort();
    }
  }

  /**
   * Get cached map data
   */
  public getCached(id: string): RawMapData | null {
    if (this.cache.has(id)) {
      this.updateCacheAccess(id);
      return this.cache.get(id)!;
    }
    return null;
  }

  /**
   * Clear cache
   */
  public clearCache(): void {
    this.cache.clear();
    this.cacheAccessOrder = [];
  }

  /**
   * Get cache statistics
   */
  public getCacheStats(): { size: number; maxSize: number; hitRate: number } {
    return {
      size: this.cache.size,
      maxSize: this.config.maxCacheSize,
      hitRate: 0, // TODO: Track hits/misses
    };
  }

  /**
   * Add map to cache (with LRU eviction)
   */
  private addToCache(id: string, mapData: RawMapData): void {
    // Evict if full
    if (this.cache.size >= this.config.maxCacheSize && !this.cache.has(id)) {
      const lruId = this.cacheAccessOrder.shift()!;
      this.cache.delete(lruId);
    }

    this.cache.set(id, mapData);
    this.updateCacheAccess(id);
  }

  /**
   * Update cache access order (LRU)
   */
  private updateCacheAccess(id: string): void {
    // Remove from current position
    const index = this.cacheAccessOrder.indexOf(id);
    if (index > -1) {
      this.cacheAccessOrder.splice(index, 1);
    }

    // Add to end (most recently used)
    this.cacheAccessOrder.push(id);
  }

  /**
   * Create batches for parallel loading
   */
  private createBatches<T>(items: T[], batchSize: number): T[][] {
    const batches: T[][] = [];
    for (let i = 0; i < items.length; i += batchSize) {
      batches.push(items.slice(i, i + batchSize));
    }
    return batches;
  }
}
```

**Usage Example**:
```typescript
const batchLoader = new BatchMapLoader({
  maxConcurrent: 3,
  maxCacheSize: 10,
  onProgress: (progress) => {
    console.log(`[${progress.taskId}] ${progress.status} - ${progress.progress}%`);
  },
});

const tasks: MapLoadTask[] = [
  { id: 'map1', file: file1, extension: '.w3x', sizeBytes: 1024000 },
  { id: 'map2', file: file2, extension: '.w3n', sizeBytes: 52428800 },
  // ... 22 more maps
];

const result = await batchLoader.loadMaps(tasks);
console.log(`Loaded ${result.stats.succeeded}/${result.stats.total} maps`);
```

---

## üß™ Validation

```bash
npm run typecheck
npm test -- src/formats/maps/BatchMapLoader.test.ts
npm run test:batch-load  # Load all 24 maps
```

**Expected**:
- ‚úÖ All 24 maps load in <2 minutes
- ‚úÖ Max 3 concurrent loads at any time
- ‚úÖ Memory usage <4GB peak
- ‚úÖ Cache eviction works correctly (LRU)
- ‚úÖ Progress callbacks fire correctly
- ‚úÖ Cancellation stops in-progress loads

---

## üì¶ Tasks (4 days)

**Day 1**: Core structure + priority queue
**Day 2**: LRU cache implementation
**Day 3**: Progress tracking + cancellation
**Day 4**: Testing with all 24 maps + optimization

---

## üö® Risks

üü° **Medium**: 923MB W3N file may cause memory spike
**Mitigation**: Use streaming (PRP 2.10), load last

üü¢ **Low**: Well-defined problem, clear performance targets

---

## üìö References

- **Pattern**: Standard batch loading with Promise.allSettled()
- **Cache**: LRU eviction algorithm
- **Priority**: Sort by size (small first for fast feedback)

---

## üéØ Confidence: **9.0/10**

Straightforward parallel loading implementation with LRU cache.
