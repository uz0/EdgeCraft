# PRP 2.10: Map Streaming System for Large Files

**Feature Name**: Chunked Streaming for 100MB+ Maps
**Duration**: 4-5 days | **Team**: 1 developer | **Budget**: $4,000
**Status**: ðŸ“‹ Planned

**Dependencies**:
- PRP 2.3 (W3NCampaignLoader) - primary use case (923MB file!)
- Phase 1 (MPQParser) - needs streaming support

---

## ðŸŽ¯ Objective

Implement streaming/chunked loading for large map files (especially the 923MB W3N campaign). Prevents browser memory crashes and provides progress feedback.

**Core Responsibility**: Load 923MB file without crashing, <15s load time

---

## ðŸ“Š Current State

**âœ… WORKING**:
- MPQParser parses files loaded into memory
- W3NCampaignLoader extracts embedded maps
- File API supports `file.arrayBuffer()` (but loads entire file)

**âŒ MISSING**:
- Streaming file reader (chunk by chunk)
- Progressive MPQ parsing (header â†’ hash â†’ block table â†’ files)
- Memory management (unload chunks after parsing)
- Progress tracking for large files

---

## ðŸ”¬ Research

**Source**: File API and streaming best practices

**Key Findings**:
1. Use `ReadableStream` API for chunked reading
2. MPQ structure supports streaming (header is first 512 bytes)
3. Parse header â†’ determine offsets â†’ seek to specific files
4. Don't load entire archive into memory at once
5. Progress: `bytesRead / totalBytes * 100`

**MPQ Structure (streamable)**:
```
[0-512] Header (magic, version, hash table offset, etc.)
[512-N] Archive data
[Offset X] Hash table
[Offset Y] Block table
[Offset Z] Individual files
```

**Strategy**:
1. Read header (512 bytes)
2. Seek to hash table â†’ read
3. Seek to block table â†’ read
4. Extract specific files on-demand (streaming)
5. Never load entire 923MB into memory

---

## ðŸ“‹ Definition of Done

- [ ] `StreamingFileReader.ts` created in `src/utils/`
- [ ] Chunk size: 4MB per read
- [ ] Progress tracking (bytes read / total)
- [ ] Cancellation support (abort stream)
- [ ] `MPQParser.parseStream()` method (streaming version)
- [ ] Integrated with W3NCampaignLoader
- [ ] Load 923MB file in <15 seconds
- [ ] Memory usage <1GB peak (not 923MB!)
- [ ] Browser doesn't crash or freeze
- [ ] Unit tests (>80% coverage)

---

## ðŸ’» Implementation

```typescript
// src/utils/StreamingFileReader.ts

export interface StreamConfig {
  /** Chunk size in bytes */
  chunkSize?: number;

  /** Progress callback */
  onProgress?: (bytesRead: number, totalBytes: number) => void;

  /** Abort signal */
  signal?: AbortSignal;
}

export interface ChunkReadResult {
  /** Chunk data */
  data: Uint8Array;

  /** Chunk offset in file */
  offset: number;

  /** Is final chunk */
  isLast: boolean;
}

export class StreamingFileReader {
  private file: File;
  private config: Required<Omit<StreamConfig, 'signal'>> & { signal?: AbortSignal };
  private position: number = 0;

  constructor(file: File, config?: StreamConfig) {
    this.file = file;
    this.config = {
      chunkSize: config?.chunkSize ?? 4 * 1024 * 1024, // 4MB
      onProgress: config?.onProgress ?? (() => {}),
      signal: config?.signal,
    };
  }

  /**
   * Read file in chunks (async generator)
   */
  public async *readChunks(): AsyncGenerator<ChunkReadResult> {
    const totalBytes = this.file.size;

    while (this.position < totalBytes) {
      // Check for cancellation
      if (this.config.signal?.aborted) {
        throw new Error('Stream aborted');
      }

      const chunkSize = Math.min(this.config.chunkSize, totalBytes - this.position);
      const blob = this.file.slice(this.position, this.position + chunkSize);
      const arrayBuffer = await blob.arrayBuffer();
      const data = new Uint8Array(arrayBuffer);

      yield {
        data,
        offset: this.position,
        isLast: this.position + chunkSize >= totalBytes,
      };

      this.position += chunkSize;
      this.config.onProgress(this.position, totalBytes);
    }
  }

  /**
   * Read specific byte range
   */
  public async readRange(offset: number, length: number): Promise<Uint8Array> {
    if (offset + length > this.file.size) {
      throw new Error('Range exceeds file size');
    }

    const blob = this.file.slice(offset, offset + length);
    const arrayBuffer = await blob.arrayBuffer();
    return new Uint8Array(arrayBuffer);
  }

  /**
   * Get file size
   */
  public getSize(): number {
    return this.file.size;
  }
}
```

**Updated MPQParser with Streaming**:
```typescript
// src/formats/mpq/MPQParser.ts (additions)

import { StreamingFileReader } from '../../utils/StreamingFileReader';

export class MPQParser {
  /**
   * Parse MPQ archive from stream (for large files)
   */
  public async parseStream(
    reader: StreamingFileReader,
    options?: {
      extractFiles?: string[]; // Only extract specific files
      onProgress?: (stage: string, progress: number) => void;
    }
  ): Promise<MPQParseResult> {
    const startTime = performance.now();

    try {
      // Step 1: Read header (512 bytes)
      options?.onProgress?.('Reading header', 0);
      const headerData = await reader.readRange(0, 512);
      const header = this.parseHeader(headerData);

      if (!this.validateHeader(header)) {
        throw new Error('Invalid MPQ header');
      }

      // Step 2: Read hash table
      options?.onProgress?.('Reading hash table', 20);
      const hashTableSize = header.hashTableEntries * 16; // 16 bytes per entry
      const hashTableData = await reader.readRange(header.hashTableOffset, hashTableSize);
      const hashTable = this.parseHashTable(hashTableData, header.hashTableEntries);

      // Step 3: Read block table
      options?.onProgress?.('Reading block table', 40);
      const blockTableSize = header.blockTableEntries * 16;
      const blockTableData = await reader.readRange(header.blockTableOffset, blockTableSize);
      const blockTable = this.parseBlockTable(blockTableData, header.blockTableEntries);

      // Step 4: Build file list
      options?.onProgress?.('Building file list', 60);
      const fileList = this.buildFileList(hashTable, blockTable);

      // Step 5: Extract specific files (if requested)
      const files: MPQFile[] = [];
      if (options?.extractFiles) {
        for (let i = 0; i < options.extractFiles.length; i++) {
          const fileName = options.extractFiles[i];
          options?.onProgress?.(
            `Extracting ${fileName}`,
            60 + (i / options.extractFiles.length) * 40
          );

          const file = await this.extractFileStream(fileName, reader, hashTable, blockTable);
          if (file) {
            files.push(file);
          }
        }
      }

      options?.onProgress?.('Complete', 100);

      return {
        success: true,
        header,
        files,
        fileList,
        parseTimeMs: performance.now() - startTime,
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
        parseTimeMs: performance.now() - startTime,
      };
    }
  }

  /**
   * Extract single file from stream
   */
  private async extractFileStream(
    fileName: string,
    reader: StreamingFileReader,
    hashTable: HashEntry[],
    blockTable: BlockEntry[]
  ): Promise<MPQFile | null> {
    const hash = this.hashFileName(fileName);
    const hashEntry = hashTable.find((h) => h.nameHash === hash);

    if (!hashEntry) {
      console.warn(`File not found: ${fileName}`);
      return null;
    }

    const blockEntry = blockTable[hashEntry.blockIndex];

    // Read compressed file data
    const compressedData = await reader.readRange(blockEntry.fileOffset, blockEntry.compressedSize);

    // Decompress
    const decompressedData = this.decompress(
      compressedData,
      blockEntry.compressionMethod,
      blockEntry.uncompressedSize
    );

    return {
      name: fileName,
      data: decompressedData,
      compressedSize: blockEntry.compressedSize,
      uncompressedSize: blockEntry.uncompressedSize,
    };
  }
}
```

**Updated W3NCampaignLoader**:
```typescript
// src/formats/maps/w3n/W3NCampaignLoader.ts (streaming version)

public async parse(file: File | ArrayBuffer): Promise<RawMapData> {
  // Detect large files
  const fileSize = file instanceof ArrayBuffer ? file.byteLength : file.size;

  if (fileSize > 100 * 1024 * 1024) {
    // >100MB - use streaming
    console.log(`Large file detected (${(fileSize / 1024 / 1024).toFixed(1)} MB), using streaming...`);
    return this.parseStreaming(file as File);
  } else {
    // <100MB - use in-memory parsing
    return this.parseInMemory(file);
  }
}

private async parseStreaming(file: File): Promise<RawMapData> {
  const reader = new StreamingFileReader(file, {
    chunkSize: 4 * 1024 * 1024,
    onProgress: (read, total) => {
      console.log(`Loading: ${((read / total) * 100).toFixed(1)}%`);
    },
  });

  const mpq = new MPQParser();
  const result = await mpq.parseStream(reader, {
    extractFiles: ['war3campaign.w3f', '*.w3x'], // Only extract what we need
    onProgress: (stage, progress) => {
      console.log(`${stage}: ${progress}%`);
    },
  });

  if (!result.success) {
    throw new Error('Failed to parse campaign');
  }

  // Extract first map and parse
  const firstMapFile = result.files.find((f) => f.name.endsWith('.w3x'));
  if (!firstMapFile) {
    throw new Error('No maps found in campaign');
  }

  return this.w3xLoader.parse(firstMapFile.data);
}
```

---

## ðŸ§ª Validation

```bash
npm run typecheck
npm test -- src/utils/StreamingFileReader.test.ts
npm run test:large-files  # Load 923MB W3N file
```

**Expected**:
- âœ… 923MB file loads in <15 seconds
- âœ… Memory usage <1GB peak (not 923MB!)
- âœ… Progress updates every chunk
- âœ… Browser doesn't freeze or crash
- âœ… Can cancel mid-stream

---

## ðŸ“¦ Tasks (5 days)

**Day 1**: StreamingFileReader implementation
**Day 2**: MPQParser.parseStream() with range reads
**Day 3**: Integration with W3NCampaignLoader
**Day 4**: Testing with 923MB file
**Day 5**: Optimization + memory profiling

---

## ðŸš¨ Risks

ðŸ”´ **High**: 923MB file is extremely large for browser
**Mitigation**: Chunk reading, on-demand extraction, aggressive GC hints

ðŸŸ¡ **Medium**: Browser File API limits
**Mitigation**: Test in Chrome/Firefox/Safari, document limitations

---

## ðŸ“š References

- **File API**: https://developer.mozilla.org/en-US/docs/Web/API/File
- **ReadableStream**: https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream
- **MPQ Format**: http://www.zezula.net/en/mpq/mpqformat.html

---

## ðŸŽ¯ Confidence: **7.5/10**

Challenging due to file size. May require browser-specific workarounds.
