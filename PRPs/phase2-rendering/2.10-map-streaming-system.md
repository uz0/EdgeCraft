# PRP 2.10: Map Streaming System for Large Files

**Feature Name**: Chunked Streaming for 100MB+ Maps
**Duration**: 4-5 days | **Team**: 1 developer | **Budget**: $4,000
**Status**: 📋 Planned

**Dependencies**:
- PRP 2.3 (W3NCampaignLoader) - primary use case (923MB file!)
- Phase 1 (MPQParser) - needs streaming support

---

## 🎯 Objective

Implement streaming/chunked loading for large map files (especially the 923MB W3N campaign). Prevents browser memory crashes and provides progress feedback.

**Core Responsibility**: Load 923MB file without crashing, <15s load time

---

## 📊 Current State

**✅ WORKING**:
- MPQParser parses files loaded into memory
- W3NCampaignLoader extracts embedded maps
- File API supports `file.arrayBuffer()` (but loads entire file)

**❌ MISSING**:
- Streaming file reader (chunk by chunk)
- Progressive MPQ parsing (header → hash → block table → files)
- Memory management (unload chunks after parsing)
- Progress tracking for large files

---

## 🔬 Research

**Source**: File API and streaming best practices

**Key Findings**:
1. Use `ReadableStream` API for chunked reading
2. MPQ structure supports streaming (header is first 512 bytes)
3. Parse header → determine offsets → seek to specific files
4. Don't load entire archive into memory at once
5. Progress: `bytesRead / totalBytes * 100`

**MPQ Structure (streamable)**:
```
[0-512] Header (magic, version, hash table offset, etc.)
[512-N] Archive data
[Offset X] Hash table
[Offset Y] Block table
[Offset Z] Individual files
```

**Strategy**:
1. Read header (512 bytes)
2. Seek to hash table → read
3. Seek to block table → read
4. Extract specific files on-demand (streaming)
5. Never load entire 923MB into memory

---

## 📋 Definition of Done

- [ ] `StreamingFileReader.ts` created in `src/utils/`
- [ ] Chunk size: 4MB per read
- [ ] Progress tracking (bytes read / total)
- [ ] Cancellation support (abort stream)
- [ ] `MPQParser.parseStream()` method (streaming version)
- [ ] Integrated with W3NCampaignLoader
- [ ] Load 923MB file in <15 seconds
- [ ] Memory usage <1GB peak (not 923MB!)
- [ ] Browser doesn't crash or freeze
- [ ] Unit tests (>80% coverage)

---

## 💻 Implementation

```typescript
// src/utils/StreamingFileReader.ts

export interface StreamConfig {
  /** Chunk size in bytes */
  chunkSize?: number;

  /** Progress callback */
  onProgress?: (bytesRead: number, totalBytes: number) => void;

  /** Abort signal */
  signal?: AbortSignal;
}

export interface ChunkReadResult {
  /** Chunk data */
  data: Uint8Array;

  /** Chunk offset in file */
  offset: number;

  /** Is final chunk */
  isLast: boolean;
}

export class StreamingFileReader {
  private file: File;
  private config: Required<Omit<StreamConfig, 'signal'>> & { signal?: AbortSignal };
  private position: number = 0;

  constructor(file: File, config?: StreamConfig) {
    this.file = file;
    this.config = {
      chunkSize: config?.chunkSize ?? 4 * 1024 * 1024, // 4MB
      onProgress: config?.onProgress ?? (() => {}),
      signal: config?.signal,
    };
  }

  /**
   * Read file in chunks (async generator)
   */
  public async *readChunks(): AsyncGenerator<ChunkReadResult> {
    const totalBytes = this.file.size;

    while (this.position < totalBytes) {
      // Check for cancellation
      if (this.config.signal?.aborted) {
        throw new Error('Stream aborted');
      }

      const chunkSize = Math.min(this.config.chunkSize, totalBytes - this.position);
      const blob = this.file.slice(this.position, this.position + chunkSize);
      const arrayBuffer = await blob.arrayBuffer();
      const data = new Uint8Array(arrayBuffer);

      yield {
        data,
        offset: this.position,
        isLast: this.position + chunkSize >= totalBytes,
      };

      this.position += chunkSize;
      this.config.onProgress(this.position, totalBytes);
    }
  }

  /**
   * Read specific byte range
   */
  public async readRange(offset: number, length: number): Promise<Uint8Array> {
    if (offset + length > this.file.size) {
      throw new Error('Range exceeds file size');
    }

    const blob = this.file.slice(offset, offset + length);
    const arrayBuffer = await blob.arrayBuffer();
    return new Uint8Array(arrayBuffer);
  }

  /**
   * Get file size
   */
  public getSize(): number {
    return this.file.size;
  }
}
```

**Updated MPQParser with Streaming**:
```typescript
// src/formats/mpq/MPQParser.ts (additions)

import { StreamingFileReader } from '../../utils/StreamingFileReader';

export class MPQParser {
  /**
   * Parse MPQ archive from stream (for large files)
   */
  public async parseStream(
    reader: StreamingFileReader,
    options?: {
      extractFiles?: string[]; // Only extract specific files
      onProgress?: (stage: string, progress: number) => void;
    }
  ): Promise<MPQParseResult> {
    const startTime = performance.now();

    try {
      // Step 1: Read header (512 bytes)
      options?.onProgress?.('Reading header', 0);
      const headerData = await reader.readRange(0, 512);
      const header = this.parseHeader(headerData);

      if (!this.validateHeader(header)) {
        throw new Error('Invalid MPQ header');
      }

      // Step 2: Read hash table
      options?.onProgress?.('Reading hash table', 20);
      const hashTableSize = header.hashTableEntries * 16; // 16 bytes per entry
      const hashTableData = await reader.readRange(header.hashTableOffset, hashTableSize);
      const hashTable = this.parseHashTable(hashTableData, header.hashTableEntries);

      // Step 3: Read block table
      options?.onProgress?.('Reading block table', 40);
      const blockTableSize = header.blockTableEntries * 16;
      const blockTableData = await reader.readRange(header.blockTableOffset, blockTableSize);
      const blockTable = this.parseBlockTable(blockTableData, header.blockTableEntries);

      // Step 4: Build file list
      options?.onProgress?.('Building file list', 60);
      const fileList = this.buildFileList(hashTable, blockTable);

      // Step 5: Extract specific files (if requested)
      const files: MPQFile[] = [];
      if (options?.extractFiles) {
        for (let i = 0; i < options.extractFiles.length; i++) {
          const fileName = options.extractFiles[i];
          options?.onProgress?.(
            `Extracting ${fileName}`,
            60 + (i / options.extractFiles.length) * 40
          );

          const file = await this.extractFileStream(fileName, reader, hashTable, blockTable);
          if (file) {
            files.push(file);
          }
        }
      }

      options?.onProgress?.('Complete', 100);

      return {
        success: true,
        header,
        files,
        fileList,
        parseTimeMs: performance.now() - startTime,
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
        parseTimeMs: performance.now() - startTime,
      };
    }
  }

  /**
   * Extract single file from stream
   */
  private async extractFileStream(
    fileName: string,
    reader: StreamingFileReader,
    hashTable: HashEntry[],
    blockTable: BlockEntry[]
  ): Promise<MPQFile | null> {
    const hash = this.hashFileName(fileName);
    const hashEntry = hashTable.find((h) => h.nameHash === hash);

    if (!hashEntry) {
      console.warn(`File not found: ${fileName}`);
      return null;
    }

    const blockEntry = blockTable[hashEntry.blockIndex];

    // Read compressed file data
    const compressedData = await reader.readRange(blockEntry.fileOffset, blockEntry.compressedSize);

    // Decompress
    const decompressedData = this.decompress(
      compressedData,
      blockEntry.compressionMethod,
      blockEntry.uncompressedSize
    );

    return {
      name: fileName,
      data: decompressedData,
      compressedSize: blockEntry.compressedSize,
      uncompressedSize: blockEntry.uncompressedSize,
    };
  }
}
```

**Updated W3NCampaignLoader**:
```typescript
// src/formats/maps/w3n/W3NCampaignLoader.ts (streaming version)

public async parse(file: File | ArrayBuffer): Promise<RawMapData> {
  // Detect large files
  const fileSize = file instanceof ArrayBuffer ? file.byteLength : file.size;

  if (fileSize > 100 * 1024 * 1024) {
    // >100MB - use streaming
    console.log(`Large file detected (${(fileSize / 1024 / 1024).toFixed(1)} MB), using streaming...`);
    return this.parseStreaming(file as File);
  } else {
    // <100MB - use in-memory parsing
    return this.parseInMemory(file);
  }
}

private async parseStreaming(file: File): Promise<RawMapData> {
  const reader = new StreamingFileReader(file, {
    chunkSize: 4 * 1024 * 1024,
    onProgress: (read, total) => {
      console.log(`Loading: ${((read / total) * 100).toFixed(1)}%`);
    },
  });

  const mpq = new MPQParser();
  const result = await mpq.parseStream(reader, {
    extractFiles: ['war3campaign.w3f', '*.w3x'], // Only extract what we need
    onProgress: (stage, progress) => {
      console.log(`${stage}: ${progress}%`);
    },
  });

  if (!result.success) {
    throw new Error('Failed to parse campaign');
  }

  // Extract first map and parse
  const firstMapFile = result.files.find((f) => f.name.endsWith('.w3x'));
  if (!firstMapFile) {
    throw new Error('No maps found in campaign');
  }

  return this.w3xLoader.parse(firstMapFile.data);
}
```

---

## 🧪 Validation

```bash
npm run typecheck
npm test -- src/utils/StreamingFileReader.test.ts
npm run test:large-files  # Load 923MB W3N file
```

**Expected**:
- ✅ 923MB file loads in <15 seconds
- ✅ Memory usage <1GB peak (not 923MB!)
- ✅ Progress updates every chunk
- ✅ Browser doesn't freeze or crash
- ✅ Can cancel mid-stream

---

## 📦 Tasks (5 days)

**Day 1**: StreamingFileReader implementation
**Day 2**: MPQParser.parseStream() with range reads
**Day 3**: Integration with W3NCampaignLoader
**Day 4**: Testing with 923MB file
**Day 5**: Optimization + memory profiling

---

## 🚨 Risks

🔴 **High**: 923MB file is extremely large for browser
**Mitigation**: Chunk reading, on-demand extraction, aggressive GC hints

🟡 **Medium**: Browser File API limits
**Mitigation**: Test in Chrome/Firefox/Safari, document limitations

---

## 📚 References

- **File API**: https://developer.mozilla.org/en-US/docs/Web/API/File
- **ReadableStream**: https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream
- **MPQ Format**: http://www.zezula.net/en/mpq/mpqformat.html

---

## 🎯 Confidence: **7.5/10**

Challenging due to file size. May require browser-specific workarounds.
